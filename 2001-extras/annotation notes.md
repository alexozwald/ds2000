**WEEK 4:  The Ethics of Using Hacked Data: Patreon's Data Hack and Academic Data Standards**

***Scholars and journalists share some functions in dealing with information and making it accessible to the public, but are the ethical considerations the same? If not, why not?***

While data scientists and journalists may follow similar ethical principles, they don't have the same job with the same implications of their work.  Poor & Davidson try to compare the sharing of 'publicly available' data from the government to digitally shared data with a case study of a NYS newspaper that "published names and addresses of gun owners in their readership area, retrieved via a freedom of information request" (3).  However, it's an ill-made comparison as 1) the accessibility of digital information is near instantaneous and drastically different from filing lengthy and meticulous forms with government and waiting for the results to be sent back; and 2) the invitation of a completely separate conversation in journalistic ethics that has explicit directives on the distinctions to classify and report on public or private citizens, minors, and citizens adjacent to a story or narrative.  Perhaps the most significant ethical consideration in data science is Big Data, which has a greater impact and sway over populations before individuals.

* * * * *

WEEK 5a: 

***That human decision loads the algorithmic profile for either male bodies or female ones, against which your body's measurements are compared***

This segment of the reading provided a particularly glaring example of the consequences of data misuse.  This example is also notably different from many others we'd see in Data Feminism because it still involves the direct interaction of a human with data-based algorithms that govern us.  For many unfamiliar or uneducated, this could be a last vestige of direct exposure to data inequity and can also serve to point to the disparity of significance placed on 'human judgement' and 'algorithm judgement,' where one is emotional or personal and the other is wholly apathetic.

By looking at an example like this it's easy, then, to imagine how behind the closed doors at a corporation like Facebook that decisions could be made to harm or classify us in ways we don't consent to all the time.  In time, I hope journalism like the recently published "facebook files" by the the WSJ with whistleblower Frances Haugen can start to turn this inequity around.

WEEK 5a: 

***In 1910, "Other" was offered as a race category for the first time, but the vast majority of those who selected it were Korean, Filipino and AsianIndian.***

This strikes me as wholly unsurprising.  The negligence of the white man will never cease to roll the eyes of history.  However, I think it also brings up an interesting point of the inclusion of an "Other: _____" response category for different questions on the census, and why it's sparsely included.  What makes the census data so important is it's direct impact on funding and local communities, and if population or demographics are miscounted (due to misunderstanding, misinterpretation, or miscount).  The biggest risk at the start of the 2020 census was that due to comments of President DJT about a citizenship question, less people were likely to fill out the census resulting in an undercount, and a lack of funding to communities that need it most.  There are now more open response questions on the census about ethnicity as a subsection of race, but definitions are still divided by letter.  There's risk in not allowing enough breadth of options on a question, but there's also risk in having partially answered questions and unstructured data from too much freedom.

* * * * *

WEEK 6a: 

***A spokeswoman for Grindr described the Pillar's story as "homophobic" and denied that the data described in it couldbe publicly accessed.***

A fascinating read.  I can't say I'm surprised grindr took this stance of defending the accusations and "outing" of the priest as homophobic; grindr is an app that notoriously provides anonymity most other dating apps do not.  However, it's a bit hypocritical in the context that grindr sold the data of their users in the first place, and didn't protect the privacy of the queer people on their apps.  The data owner can't weild a double edged sword and claim immunity to it.

WEEK 6b:

***For individual consumers, the value of constant tracking is less tangible.***

This quotation brings up a  mantra surrounding modern surveillance capitalism:  "If you're not paying for it, you're the product."  Without having solid ground in our consumer-privacy rights,  companies can do whatever they with with our information down to tracking us and judging variations in populations and making inferences from it, or surveilling employees taking interviews at another company which was also revealed in this article.  Google & Microsoft (and to a lesser degree Apple) all provide free services we depend on from maps to google docs and all of their educational services used by minors -- leaving little wiggle room in modern society to find an alternative.  If this massive trove was the biggest ever 'gifted to the Times', how big are Google's data files on all of us?  How many petabytes of data does Facebook collect on people, and people without Facebook accounts?

WEEK 6c:

***"Given the delay in delivery of the redistricting data,however, we must prioritize production of the redistricting data so that states cancommence already-delayed redistricting work."***

While perhaps annoying to congress, I admire the Census for taking the proper time to anonymize their data.  People oft think of the raw US Census data as possibly one of the most widely sought after datasets in the world for it's unprecedented impact on US politics and life.  However, is the Census Bureau data really more valuable or even accurate  than the data collected by the likes of Google, Facebook, & Amazon?

The implications of this question also provides some similarity to the concurrent 6b reading "One Nation, Tracked" by the NYT.  Both stories focus on the accessibility and sheer scale & detail of data.  Does the US Census really know more about us than Google, who is better funded, more consistent in their tracking (constant vs 10-yr intervals), and who spends more resources and time on refining the data.  The only significant difference between the two is method of collection - Google's is all digital and digital-adjacent where the government collects data door-to-door.  But in the modern age, how much does foot-traffic really count?  The discrepancy between the 2020 election polls and election results might say quite a bit, but not enough do discount this shadow of doubt--is Google smarter than the government?

* * * * *

WEEK 7a:

***calculations that transformed the underlying data into a score are rarely revealed***

**In theory, risk assessment could have positive implications. If it was done fairly, it could be useful in assigning people certain types of sentences or prohibition, but at this point it is deeply flawed. Even if the scores are not calculated directly based on race, they clearly fail to account for racial differences and inequalities in society. The fact that the data and calculations is confidential is really suspicious to me. I also wonder how often people lie, as I am sure they understand the point of the assessment and would not necessarily endorse stealing, for example, when questioned by someone involved in law enforcement. I think the judge and jury should be aware of the types of questions asked, and even which questions raised a defendant's score. That being said, I am not sure that is actually ethical given our previous discussions on data security. Perhaps the best option is just for the questions asked and the calculations used to be made public, while the responses are kept private. Honestly I am not sure the best course of action here in terms of data privacy, but this method of rating surely needs to become more transparent.**

[response]

I agree with Piper here,  risk assessment definitely could yield some positive results if implemented properly, responsibly, and by the right parties.  One of the most important risk factors to properly be considered should be financial mobility, which in the US is directly disproportional to black skin color.  A better way to implement this could be to open-source the code & foundational data the justice department was using rather than allowing for proper public criticism and contribution.  Yet, the most significant challenge to using "open source governing" methods lies in contribution... where lawmakers being the only ones allowed to push a change would be very undemocratic, and yet a directly democratic approach would be extremely difficult to implement on such a large scale and is wholly un-American, and does not properly define lawmakers but takes a more communistic decision contribution model.  

Week 7b:

***While organizing this book, I have wanted to emphasize one main point: there is a missing social and human context in some types of algorithmically driven decision making, and this matters for every-one engaging with these types of technologies in everyday life.***

**This is an especially important point, because it captures the upper limit of technology and machine learning, which is just that, a machine. Humans interact in so many ways that cannot be captured quantifiably. Understanding this and making sure that we are aware of the scope of algorithms is important when decided what is feasible. It is also just as important for those who interpret and use the data to understand, since it is in all parts of data. Also understanding that these effects are not uniform among groups, and they disproportionately effect marginalized groups, like most social issues do.**

I wholeheartedly agree here, I think this is the main point behind the authors discredit of digital redlining.  The potential impact of algorithms and unsupervised decisions on human life could be insurmountable for those not in a position of power.  Many fear the day 'robots take over' but the day algorithms do will be worse for it's a self-inflicted wound.  The decision of what data to use in algorithms is of great consequence, as is the model they're trained with.  Human-based decisions are contextual and take into account an ethos appeal whereas a machine couldn't comprehend or compute any of those values.  Grace and forgiveness are essential human qualities machines lack, vengeance isn't the only decision to fear.

* * * * *

WEEK 8a:

***Twitter's ML Ethics, Transparency, and Accountability(META) Team***

I've previously listened to a few podcasts that covered Twitter's cropping algorithm scandal, but none previously had mentioned the forming of this META Team.  In the article's disambiguation of it, I'm not surprised that Twitter didn't answer who would "make the final call" since corporate bodies rarely declare doctrines with such finality before finding out exactly what the team or new policy's findings are.  That said, I still think it's more hopeful they drafted the team in the first place.  I'm curious to see whether they'll adopt an open blog policy of some of their findings similar to Netflix's netflixtechblog.com.

WEEK 8b:

***that a user's activity both on and oFacebook sends data to the platform that is used to infer interests.***

I found this line to be an interesting confirmation of the breadth of Facebook's data collection pipelines.  Facebook clearly took the route of maintaining "political correctness" with this change to their ad-targeting tools, which shows a clear and continued lack of care for the impacts of their platforms on society overall.  One thing I think this article is missing would be a useful comparison to the offerings of Google's ad-targeting tools and whether they allow a user to directly or similarly specify users to advertise to by race?  In light of Frances Haugen's recent testimony to congress, I'd hope the trove of documents she shared with the SEC is released, and whether or not Facebook's racial profiling played a large role in their "civic engagement" team and overall management of unrest on the platform.

* * * * *

WEEK 9a:

***...thus quadrant 4 deserves more attention than it has received so far.***

As an article published in Nature I read this piece with high regard from the onset and it did not disappoint.  In table 1 they showed an interesting collection of items where Quadrant 1 seemingly built up to quadrants 2 & 3 and those in turn built up to Quadrant 4 which would provide a full, thorough analysis ideally balanced.  However, this heavily hinges on seemingly an academic audience.  While a remarkable integration proposal, and a refined direction for the academic data science community, I find it hard to believe most businesses would enact this towards their research models before enacting say, a change on user privacy decisions or classifications at "Meta" where things like A/B testing might take precedence.  That said, I think for particular research purposes, extending to economists (as exemplified), policy makers, and think tanks this would absolutely be the right route.

* * * * *

WEEK 10a:

***Perhaps the most peculiar feature of the dataset is the fact that the baseline data forCar #1 in the posted Excel file appears in two different fonts.***

The discovered font discrepancy is perhaps the most fascinating piece of this investigation to me.  It's such a niche discovery and accusation to make, but at the same time is near irrefutable.  If the data was collected, duplicated, or cleaned in a manner that wasn't stated then there are few and far between alternative explanations for this.  This combined with the near-duplicate results for cars 1 and 4 provides an even further amount of hesitancy.

WEEK 10b:

*Future scholars, with only your publication and other information you provide, ought to be able to start from the real world and arrive at the same substantive conclusions. In many types of research this is not possi- ble, but it should always b*

***Being able to have your study replicatied by future scholars is so important because more data leading to the same conclusion further proves the original conclusion. The more and more evidence is collected to support the same conclusion, the stronger the argument for the conclusion, especially if the replication is conducted by different researchers than the original ones. Even if they're replicating the same study, having different researchers collect the data can help eliminate possible bias that the original researchers may have inadvertently caused.***

I think this is an interesting response to the paper's reiteration of a basic scientific principle---replication of research.  Research isn't verifiable nor certain if no one else can find a reason to trust it, I think one of the primary things missing here is the credit given to peer review.  While I concur that other researchers should be able to make the same conclusions with the same evidence or more, when more evidence is added there are unequivocally more possibilities, which is where peer review comes in.  Even analyzing the same data with two types of analyses aimed at reaching the same conclusions may diverge where they seemingly shouldn't, which is where peer review of journals comes in.  Peer review also provides another layer of security where experimental replication is difficult.

Week 10c:

***Three research teams independently tried to replicate the effect Bem had reported and, when they could not, they faced serious obstacles to publishing their results.***

What I like most about this article aside from the graphic design is the motion of the story Yong is telling through this article.  Rather than directly inform on the disproportionality of 'positive results' Yong gives us the reason this issue arose with cited examples and conducts effectively a brief meta-analysis of studies that evaluated the rising proportion of positive results or finding that most published research findings are false.  Continuing, he goes on to evaluate the relative subjectivity of psychology that could have been the basis for this migration in the field.  It also stands that this 20% (significant but not outrageous) increase over 1990--2007 in 'positive results' papers could have arisen similarly to and alongside the internet effect on younger generations to develop more 'groupthink' and accept others' ideas as readily as their own due to such a substantial quantity of information.  It's also not unreasonable to expect 

Yet still, among the graphics displayed on page 3 I expected space science to receive the least positive literature analysis (as displayed) because it's an incredibly difficult discipline that fosters much and thorough discourse about the origins of our universe.  While materials science is the most 'grounded' of all the disciplines and is the easiest to conduct confirmation research on with the least variability in experiments leading to a simple control variable as the most popularly agreeable discipline...  aside from psychology that seems to have outshone it.  Overall I appreciated the article but feel like more thought could have been given to the outlook that there's incentive to publish confirmatory articles to increase their *H-index *and Citation count, an issue inclusive to psychology.

* * * * *

WEEK 11a:

***April 4, 2018***

While dancing around the topic, I think it's still important to acknowledge that this API closure was directly in the wake of the Cambridge Analytica Scandal.  What few realized at the time was that Facebook was not breached by Cambridge Analytica, but they were exploiting a feature from the Facebook API;  the API somewhat indirectly provided access for people and companies to collect data not only on users but for all of those users' friends on Facebook.  I understand the author's commitment to topically discuss the impact this API change made for researchers and students alike, but they're just demographic of the whole world that had access to abuse the privacy nightmare that was the Facebook API.

WEEK 11b:

***The  research  community  would  greatly  benefit  from  WhatsApp ***

***disclosing the following information:***

I appreciated this article for exploring such a wide and detailed breadth of the avenues misinformation can be explored, how, and where.  But to the WhatsApp point I'm confused as to what this proposed alternative is.  WhatsApp claims to be end-to-end encrypted (bar private groups), yet it has always seemed suspicious to me that it was owned by Facebook and they didn't take full advantage of this plethora of data?  Is it within Facebook's capacity to decrypt message data safely for researchers?  If they don't have access, how then have they been "limiting the spread of content they consider to be viral" beyond the public groups researchers already have access to?

* * * * *

WEEK 12a:

***QWERTY...***

QWERTY...